# 논문 핵심 정리

## 개요

- 이 논문은 연합 학습의 주요 프라이버시 위협인 그래디언트 역전 공격(Gradient Inversion Attacks, 이하 GIA)에 대한 포괄적인 분석과 광범위한 실험을 수행한다.

- 연합 학습은 원본 데이터를 공유하지 않고 모델의 그래디언트만 공유하여 프라이버시를 보호하는 것을 목표로 한다. 하지만 이 논문은 공유된 그래디언트만으로도 원본 학습 데이터를 복원할 수 있음을 보이며, 기존 GIA 공격들을 3가지 유형으로 체계화하고 각각의 실용성과 한계를 실험적으로 검증한다.

- 저자들은 대부분의 GIA 공격이 *실용성이 낮거나(impractical)*, *성능이 불만족스럽거나(unsatisfactory)*, *쉽게 탐지 가능(easily detectable)*하다고 결론을 내린다. 이를 바탕으로, 복잡한 암호화 기술 없이도 연합 학습 설계를 최적화하여 GIA를 효과적으로 방어할 수 있는 3단계 파이프라인을 제안한다.

---

## 주요 연구 질문

- 저자들은 다음 세 가지 핵심 질문을 기반으로 연구를 진행하였다:
1. R1: GIA 공격 성능에 영향을 미치는 결정적인 요인들은 무엇인가?
    - 배치 크기, 해상도, 모델 상태 등
2. R2: 다양한 GIA 유형 중 어떤 것이 가장 실용적이며, 연합 학습에 가장 큰 위협이 되는가?
3. R3: PEFT(파라미터 효율적 파인튜닝, 예: LoRA)가 적용된 연합 학습 환경에서의 프라이버시 유출 수준은 어떠한가?

---

## GIA의 3가지 분류

- 논문은 GIA 공격을 다음과 같이 세가지 유형으로 분류한다.(논문 속 그림 1 참고):

1. 최적화 기반 GIA (OP-GIA):
    - 작동 방식: 무작위로 생성한 '더미 데이터(dummy data)'를 모델에 입력하여 그래디언트를 계산하고, 이 그래디언트가 유출된 실제 그래디언트와 일치하도록 더미 데이터를 최적화(업데이트)하여 원본을 복원한다.
    - 주요 방법: DLG, IDLG, GradInversion 등.

2. 생성 기반 GIA (GEN-GIA):
    - 작동 방식: GAN과 같은 생성자를 사용하여 원본 데이터를 재구성한다.
    - 세부 유형:
        - Latent Vector 최적화: 사전 학습된 생성자의 입력 벡터(z)를 최적화.
        - Generator Parameter 최적화: 생성자 모델의 가중치(W)를 최적화
        - Inversion Model 학습: 그래디언트에서 데이터로 변환하는 별도의 모델을 학습

3. 분석 기반 GIA (ANA-GIA):
    - 작동 방식: 악의적인 서버가 클라이언트에게 보내는 모델 아키텍처나 파라미터를 의도적으로 조작하여, 그래디언트에서 원본 데이터를 수학적으로(closed-form) 쉽게 계산할 수 있도록 만든다.
    - 세부 유형:
        - 모델 아키텍처 조작: 'Robbing the Fed'처럼 특정 레이어를 삽입
        - 모델 파라미터 조작: 'Fishing'처럼 모델 가중치를 조작

---

## 주요 실험 결과 및 분석

각 GIA 유형의 실용성과 위협 수준을 평가한 결과는 다음과 같다.

1. 최적화 기반 (OP-GIA)
    - (Takeaway 1) 가장 실용적이지만 성능이 불만족스럽다.
    - 실용성: 별도의 보조 데이터셋이나 사전 학습된 생성자가 필요 없어 가장 실용적이다.
    - 한계: 성능이 여러 요인에 크게 저하된다.
        - 배치 크기(Batch Size): 배치 크기가 클수록 성능이 급격히 저하된다.
        - 이미지 해상도(Resolution): 고해상도 이미지(예: ImageNet)는 복원이 거의 불가능하다.
        - 모델 상태: 모델이 잘 학습될수록 그래디언트가 유사해져 복원이 더 어려워진다.
        - 네트워크: 더 복잡한 네트워크(예: ResNet-18 < ResNet-101)일수록 성능이 저하된다.
    - 핵심: 여러 로컬 스텝을 학습하는 실용적인 FedAvg 방식은 그 자체로 OP-GIA에 대한 방어 능력을 가진다.

2. 생성 기반 (GEN-GIA)
    - (Takeaway 2) 의존성이 많아 위협이 거의 되지 않는다.
    - Latent Vector 최적화: 픽셀 단위의 정확한 복원이 아닌 '의미론적으로 유사한' 이미지만 생성 가능하며, 사전 학습된 생성자에 크게 의존한다.
    - Generator Parameter 최적화: 픽셀 수준 복원이 가능하지만, 실험 결과 Sigmoid 활성화 함수를 사용하는 모델에서만 작동한다. (현대 딥러닝 모델은 대부분 ReLU, GeLU 등을 사용하므로 비현실적이다.)
    - Inversion Model 학습: 그래디언트-이미지 매핑을 학습하기 위해 원본 데이터와 분포가 유사한 보조 데이터셋이 필요해 비현실적이다.

3. 분석 기반 (ANA-GIA)
    - (Takeaway 3) 성능은 우수하지만, 쉽게 탐지 가능하다.
    - 성능: 배치 크기나 해상도에 거의 영향을 받지 않고 거의 완벽한 데이터 복원이 가능하다.
    - 한계 (비실용성): 공격을 위해 서버가 모델 아키텍처나 파라미터를 악의적으로 수정해야 한다. 이는 클라이언트가 모델을 수신할 때 간단한 검증만으로도 즉시 탐지할 수 있어 매우 비현실적이다.

4. PEFT (LoRA) 환경에서의 공격 (R3와 연관)
    - (Takeaway 4) 저해상도 이미지는 유출되지만 고해상도는 안전하다.
    - 실험 결과, LoRA 파라미터의 그래디언트만으로도 저해상도 이미지(CIFAR-10, CelebA 등)는 복원이 가능했다.
    - 하지만 고해상도 이미지(ImageNet)는 복원에 실패했다.
    - 흥미롭게도, 더 작은 사전 학습 모델(Vit-tiny)이 더 큰 모델(ViT-large)보다 프라이버시 보호에 유리했다. (LoRA 파라미터 수가 적어 유출되는 정보량이 적기 때문)

---

## 결론: 3단계 방어 가이드라인 (Takeaway 5)

- 저자들은 현재 GIA 공격이 모두 명확한 한계를 지니며, 복잡한 암호화 기술 없이도 연합 학습 프로토콜 설계를 신중하게 하는 것만으로도 데이터 프라이버시를 크게 향상시킬 수 있다고 결론 내린다.

- 이들이 제안하는 3단계 방어 파이프라인은 다음과 같다:

1. 1단계(네트워크 설계):
    - Sigmoid 활성화 함수 사용을 피한다. (GEN-GIA 방어)
    - 더 복잡한 네트워크 아키텍처를 사용한다. (OP-GIA 방어)

2. 2단계 (로컬 학습 프로토콜):
    - 더 큰 배치 크기를 사용한다. (OP-GIA 방어)
    - 그래디언트를 한 번만 계산하지 않고, 여러 로컬 스텝(multi-step local training)을 수행한다. (즉, FedSGD 대신 FedAvg 사용) (OP-GIA 방어)
    
3. 3단계 (클라이언트 측 검증):
    - 서버로부터 모델을 받을 때, 모델 아키텍처나 파라미터가 악의적으로 수정되지 않았는지 검증한다. (ANA-GIA 방어)